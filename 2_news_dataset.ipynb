{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb604ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ê²°ê³¼ ë°ì´í„°] ìƒìœ„ 5ê°œ:\n",
      "  news_id        publish_time\n",
      "0  N10005 2019-11-09 09:20:40\n",
      "1  N10007 2019-11-14 08:46:12\n",
      "2  N10008 2019-11-15 12:28:50\n",
      "3  N10010 2019-11-13 10:33:08\n",
      "4  N10011 2019-11-11 04:28:37\n"
     ]
    }
   ],
   "source": [
    "# impressionsì˜ ê²Œì‹œëœ ì‹œê°„ ì¶”ì¶œ\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "BEHAVIOR_COLUMNS = ['ImpressionID', 'UserID', 'Time', 'History', 'Impressions']\n",
    "DATE_FORMAT = \"%m/%d/%Y %I:%M:%S %p\"\n",
    "\n",
    "def load_behaviors(train_path: str, dev_path: str) -> pd.DataFrame:\n",
    "    train = pd.read_csv(train_path, sep='\\t', names=BEHAVIOR_COLUMNS, header=0)\n",
    "    dev = pd.read_csv(dev_path, sep='\\t', names=BEHAVIOR_COLUMNS, header=0)\n",
    "    return pd.concat([train, dev], ignore_index=True)\n",
    "\n",
    "def extract_earliest_times(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.dropna(subset=['Impressions', 'Time'])\n",
    "    df['Time'] = pd.to_datetime(df['Time'], format=DATE_FORMAT, errors='coerce')\n",
    "    df = df.dropna(subset=['Time'])\n",
    "\n",
    "    # ë‰´ìŠ¤ IDë§Œ ì¶”ì¶œ (ìˆ«ì ë¶€ë¶„ ì œê±°)\n",
    "    df['ImpressionList'] = df['Impressions'].str.split().apply(\n",
    "        lambda imps: [imp.split('-')[0] for imp in imps if '-' in imp and imp.split('-')[0] != '']\n",
    "    )\n",
    "\n",
    "    exploded = df[['Time', 'ImpressionList']].explode('ImpressionList')\n",
    "    exploded.rename(columns={'ImpressionList': 'news_id'}, inplace=True)\n",
    "\n",
    "    result = exploded.groupby('news_id', as_index=False)['Time'].min().rename(columns={'Time': 'publish_time'})\n",
    "    print(\"[ê²°ê³¼ ë°ì´í„°] ìƒìœ„ 5ê°œ:\")\n",
    "    print(result.head())\n",
    "    return result\n",
    "\n",
    "def save_to_csv(df: pd.DataFrame, path: str):\n",
    "    df.to_csv(path, sep='\\t', index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    behaviors = load_behaviors(\n",
    "        'download/MINDsmall_train/behaviors.tsv',\n",
    "        'download/MINDsmall_dev/behaviors.tsv'\n",
    "    )\n",
    "    result = extract_earliest_times(behaviors)\n",
    "    save_to_csv(result, 'news_publish_times.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "203a2a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_24564\\835834188.py:12: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  behaviors_df['Time'] = pd.to_datetime(behaviors_df['Time'])\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230115/230115 [00:05<00:00, 39810.13it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "BEHAVIOR_COLUMNS = ['ImpressionID', 'UserID', 'Time', 'History', 'Impressions']\n",
    "train = pd.read_csv('download/MINDsmall_train/behaviors.tsv', sep='\\t', names=BEHAVIOR_COLUMNS, header=0)\n",
    "dev = pd.read_csv('download/MINDsmall_dev/behaviors.tsv', sep='\\t', names=BEHAVIOR_COLUMNS, header=0)\n",
    "behaviors_df = pd.concat([train, dev], ignore_index=True)\n",
    "\n",
    "behaviors_df['Time'] = pd.to_datetime(behaviors_df['Time'])\n",
    "\n",
    "news_time = defaultdict(list)\n",
    "\n",
    "for _, row in tqdm(behaviors_df.iterrows(), total=len(behaviors_df)):\n",
    "    time = row['Time']\n",
    "    impressions = str(row['Impressions']).split()\n",
    "    for imp in impressions:\n",
    "        news_id = imp.split('-')[0] \n",
    "        news_time[news_id].append(time)\n",
    "\n",
    "publish_time = {}\n",
    "\n",
    "for news, times in news_time.items():\n",
    "    publish_time[news] = min(times)\n",
    "\n",
    "publish_df = pd.DataFrame([\n",
    "    {'news_id': news_id, 'publish_time': time.strftime('%m/%d/%Y %I:%M:%S %p')}\n",
    "    for news_id, time in publish_time.items()\n",
    "])\n",
    "\n",
    "publish_df.to_csv('news_publish_time.tsv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0f63e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_expand_news.py\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "BEHAVIOR_COLUMNS = ['ImpressionID', 'UserID', 'Time', 'History', 'Impressions']\n",
    "DATE_FORMAT = \"%m/%d/%Y %I:%M:%S %p\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 1. ë‰´ìŠ¤ë³„ publish_time ë¡œë“œ ë° end_time ê³„ì‚°\n",
    "    publish_df = pd.read_csv('news_publish_times.csv')\n",
    "    publish_df['publish_time'] = pd.to_datetime(publish_df['publish_time'])\n",
    "    publish_df['end_time'] = publish_df['publish_time'] + timedelta(hours=24)\n",
    "\n",
    "    # 2. expanded_behaviors.csv ë¡œë“œ\n",
    "    expanded_df = pd.read_csv('expanded_behaviors.csv', sep='\\t')\n",
    "    expanded_df['Time'] = pd.to_datetime(expanded_df['Time'], format=DATE_FORMAT, errors='coerce')\n",
    "    expanded_df.dropna(subset=['Time'], inplace=True)\n",
    "\n",
    "    # 3. ê° rowë³„ë¡œ end_time > Time ì¸ news_idë§Œ í•„í„°ë§í•˜ì—¬ 20ê°œ ëœë¤ ìƒ˜í”Œë§\n",
    "    for i in range(len(expanded_df)):\n",
    "        row_time = expanded_df.at[i, 'Time']\n",
    "\n",
    "        valid_news_df = publish_df[publish_df['end_time'] > row_time]\n",
    "        valid_news = valid_news_df['news_id'].tolist()\n",
    "\n",
    "        if len(valid_news) < 20:\n",
    "            sampled = valid_news\n",
    "        else:\n",
    "            sampled = random.sample(valid_news, 20)\n",
    "\n",
    "        # ê¸°ì¡´ Impressions + ìƒ˜í”Œ ì¶”ê°€\n",
    "        old_val = str(expanded_df.at[i, 'Impressions'])\n",
    "        new_val = f\"{old_val} {' '.join([f'{nid}-0' for nid in sampled])}\"\n",
    "        expanded_df.at[i, 'Impressions'] = new_val\n",
    "\n",
    "    # 4. ì €ì¥\n",
    "    expanded_df.to_csv('news_dataset.csv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4fdc45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8236715/8236715 [01:02<00:00, 131236.33it/s]\n",
      "Generating impressions:   9%|â–Š         | 30323/347727 [1:03:15<11:02:13,  7.99it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 77\u001b[0m\n\u001b[0;32m     75\u001b[0m expanded_rows \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m tqdm(user_behavior_rows, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGenerating impressions\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 77\u001b[0m     sampled \u001b[38;5;241m=\u001b[39m zero_df_filtered\u001b[38;5;241m.\u001b[39msample(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;28mlen\u001b[39m(zero_df_filtered)), random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     78\u001b[0m     zero_imps \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m nid \u001b[38;5;129;01min\u001b[39;00m sampled[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNewsID\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[0;32m     79\u001b[0m     full_impression \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclicked_news\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzero_imps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:6118\u001b[0m, in \u001b[0;36mNDFrame.sample\u001b[1;34m(self, n, frac, replace, weights, random_state, axis, ignore_index)\u001b[0m\n\u001b[0;32m   6115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   6116\u001b[0m     weights \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mpreprocess_weights(\u001b[38;5;28mself\u001b[39m, weights, axis)\n\u001b[1;32m-> 6118\u001b[0m sampled_indices \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39msample(obj_len, size, replace, weights, rs)\n\u001b[0;32m   6119\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(sampled_indices, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m   6121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_index:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\core\\sample.py:152\u001b[0m, in \u001b[0;36msample\u001b[1;34m(obj_len, size, replace, weights, random_state)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    150\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weights: weights sum to zero\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m random_state\u001b[38;5;241m.\u001b[39mchoice(obj_len, size\u001b[38;5;241m=\u001b[39msize, replace\u001b[38;5;241m=\u001b[39mreplace, p\u001b[38;5;241m=\u001b[39mweights)\u001b[38;5;241m.\u001b[39mastype(\n\u001b[0;32m    153\u001b[0m     np\u001b[38;5;241m.\u001b[39mintp, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    154\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# tqdmì„ pandasì˜ applyì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •\n",
    "tqdm.pandas()\n",
    "\n",
    "# 1. í•™ìŠµ ë° ê²€ì¦ behaviors ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "train_behaviors = pd.read_csv(\n",
    "    'download/MINDsmall_train/behaviors.tsv',\n",
    "    sep='\\t',\n",
    "    names=['ImpressionID', 'UserID', 'Time', 'History', 'Impressions']\n",
    ")\n",
    "\n",
    "dev_behaviors = pd.read_csv(\n",
    "    'download/MINDsmall_dev/behaviors.tsv',\n",
    "    sep='\\t',\n",
    "    names=['ImpressionID', 'UserID', 'Time', 'History', 'Impressions']\n",
    ")\n",
    "\n",
    "# 2. í•™ìŠµ/ê²€ì¦ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ í†µí•©\n",
    "df = pd.concat([train_behaviors, dev_behaviors], ignore_index=True)\n",
    "\n",
    "# 3. í´ë¦­í•œ ë‰´ìŠ¤ì™€ ì›ë˜ í–‰ë™ ì •ë³´ ë¶„ë¦¬ ì €ì¥\n",
    "user_behavior_rows = []\n",
    "zero_label_news_ids = set()\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    impression_id, user_id, time_str, history, impressions = row\n",
    "    time = datetime.strptime(time_str, \"%m/%d/%Y %I:%M:%S %p\")\n",
    "    click_news = []\n",
    "    for item in impressions.split():\n",
    "        news_id, label = item.split('-')\n",
    "        if label == '1':\n",
    "            click_news.append(f\"{news_id}-1\")\n",
    "        elif label == '0':\n",
    "            zero_label_news_ids.add(news_id)\n",
    "\n",
    "    for clicked in click_news:\n",
    "        user_behavior_rows.append({\n",
    "            'ImpressionID': impression_id,\n",
    "            'UserID': user_id,\n",
    "            'Time': time,\n",
    "            'History': history,\n",
    "            'clicked_news': clicked\n",
    "        })\n",
    "\n",
    "# 4. -0 ë‰´ìŠ¤ëŠ” ì¶”ì²œëœ ì‹œê°ê³¼ í•¨ê»˜ ì €ì¥\n",
    "zero_label_records = []\n",
    "for _, row in df.iterrows():\n",
    "    time = datetime.strptime(row['Time'], \"%m/%d/%Y %I:%M:%S %p\")\n",
    "    for item in row['Impressions'].split():\n",
    "        news_id, label = item.split('-')\n",
    "        if label == '0':\n",
    "            zero_label_records.append({'NewsID': news_id, 'Time': time})\n",
    "\n",
    "zero_df = pd.DataFrame(zero_label_records)\n",
    "\n",
    "# 5. ë‰´ìŠ¤ ë°œí–‰ ì‹œê°„ ë¶ˆëŸ¬ì˜¤ê¸° ë° datetime ë³€í™˜\n",
    "news_df = pd.read_csv('news_publish_times.csv')\n",
    "news_df['publish_time'] = pd.to_datetime(news_df['publish_time'])\n",
    "news_publish_map = dict(zip(news_df['news_id'], news_df['publish_time']))\n",
    "\n",
    "# 6. (ì œê±°ë¨) ê° ë‰´ìŠ¤ ID ê¸°ì¤€ ìµœì‹  í–‰ë™ ì‹œê°„ ê³„ì‚° ë¶ˆí•„ìš” â†’ ë°”ë¡œ row['Time'] í™œìš© ê°€ëŠ¥\n",
    "def within_24_hours(row):\n",
    "    pub_time = news_publish_map.get(row['NewsID'])\n",
    "    return pub_time is not None and row['Time'] <= pub_time + timedelta(hours=24)\n",
    "\n",
    "# tqdm ì ìš©í•˜ì—¬ í•„í„°ë§ ì²˜ë¦¬ ì§„í–‰ë¥  í™•ì¸\n",
    "zero_df['valid'] = zero_df.progress_apply(within_24_hours, axis=1)\n",
    "zero_df_filtered = zero_df[zero_df['valid']].drop(columns='valid')\n",
    "\n",
    "# 8. í´ë¦­ ë‰´ìŠ¤ì™€ -0 ë‰´ìŠ¤ 20ê°œë¥¼ ë¬¶ì–´ì„œ í•˜ë‚˜ì˜ impressions í•„ë“œë¡œ ìƒì„±\n",
    "expanded_rows = []\n",
    "for row in tqdm(user_behavior_rows, desc='Generating impressions'):\n",
    "    sampled = zero_df_filtered.sample(n=min(20, len(zero_df_filtered)), random_state=None)\n",
    "    zero_imps = \" \".join([f\"{nid}-0\" for nid in sampled['NewsID']])\n",
    "    full_impression = f\"{row['clicked_news']} {zero_imps}\"\n",
    "    expanded_rows.append({\n",
    "        'ImpressionID': row['ImpressionID'],\n",
    "        'UserID': row['UserID'],\n",
    "        'Time': row['Time'],\n",
    "        'History': row['History'],\n",
    "        'impressions': full_impression\n",
    "    })\n",
    "\n",
    "# 9. ìµœì¢… ê²°ê³¼ DataFrame ìƒì„±\n",
    "expanded_df = pd.DataFrame(expanded_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33cbc125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating impressions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 347727/347727 [6:08:44<00:00, 15.72it/s]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# tqdmì„ pandasì˜ applyì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •\n",
    "tqdm.pandas()\n",
    "\n",
    "# 1. í•™ìŠµ ë° ê²€ì¦ behaviors ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "train_behaviors = pd.read_csv(\n",
    "    'download/MINDsmall_train/behaviors.tsv',\n",
    "    sep='\\t',\n",
    "    names=['ImpressionID', 'UserID', 'Time', 'History', 'Impressions']\n",
    ")\n",
    "\n",
    "dev_behaviors = pd.read_csv(\n",
    "    'download/MINDsmall_dev/behaviors.tsv',\n",
    "    sep='\\t',\n",
    "    names=['ImpressionID', 'UserID', 'Time', 'History', 'Impressions']\n",
    ")\n",
    "\n",
    "# 2. í•™ìŠµ/ê²€ì¦ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ í†µí•©\n",
    "df = pd.concat([train_behaviors, dev_behaviors], ignore_index=True)\n",
    "\n",
    "# 3. í´ë¦­í•œ ë‰´ìŠ¤ì™€ ì›ë˜ í–‰ë™ ì •ë³´ ë¶„ë¦¬ ì €ì¥ + -0 ë‰´ìŠ¤ ID ìˆ˜ì§‘\n",
    "user_behavior_rows = []\n",
    "zero_label_news_ids = set()\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    impression_id, user_id, time_str, history, impressions = row\n",
    "    time = datetime.strptime(time_str, \"%m/%d/%Y %I:%M:%S %p\")\n",
    "    click_news = []\n",
    "    for item in impressions.split():\n",
    "        news_id, label = item.split('-')\n",
    "        if label == '1':\n",
    "            click_news.append(f\"{news_id}-1\")\n",
    "        elif label == '0':\n",
    "            zero_label_news_ids.add(news_id)\n",
    "\n",
    "    for clicked in click_news:\n",
    "        user_behavior_rows.append({\n",
    "            'ImpressionID': impression_id,\n",
    "            'UserID': user_id,\n",
    "            'Time': time,\n",
    "            'History': history,\n",
    "            'clicked_news': clicked\n",
    "        })\n",
    "\n",
    "# 4. ë‰´ìŠ¤ ë°œí–‰ ì‹œê°„ ë¶ˆëŸ¬ì˜¤ê¸° ë° datetime ë³€í™˜\n",
    "news_df = pd.read_csv('news_publish_times.csv')\n",
    "news_df['publish_time'] = pd.to_datetime(news_df['publish_time'])\n",
    "news_publish_map = dict(zip(news_df['news_id'], news_df['publish_time']))\n",
    "\n",
    "# 5. í•¨ìˆ˜: íŠ¹ì • ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ 24ì‹œê°„ ë‚´ ë‰´ìŠ¤ë§Œ ë°˜í™˜\n",
    "def get_valid_zero_news_within_24h(current_time):\n",
    "    return [\n",
    "        nid for nid in zero_label_news_ids\n",
    "        if (\n",
    "            nid in news_publish_map and\n",
    "            news_publish_map[nid] <= current_time <= news_publish_map[nid] + timedelta(hours=24)\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# í´ë¦­ ë‰´ìŠ¤ì™€ -0 ë‰´ìŠ¤ 20ê°œë¥¼ ë¬¶ì–´ì„œ í•˜ë‚˜ì˜ impressions í•„ë“œë¡œ ìƒì„±\n",
    "expanded_rows = []\n",
    "for row in tqdm(user_behavior_rows, desc='Generating impressions'):\n",
    "    time = row['Time']\n",
    "    candidate_news = get_valid_zero_news_within_24h(time)\n",
    "    sampled = random.sample(candidate_news, k=min(20, len(candidate_news)))\n",
    "    zero_imps = \" \".join([f\"{nid}-0\" for nid in sampled])\n",
    "    full_impression = f\"{row['clicked_news']} {zero_imps}\"\n",
    "\n",
    "    expanded_rows.append({\n",
    "        'ImpressionID': row['ImpressionID'],\n",
    "        'UserID': row['UserID'],\n",
    "        'Time': row['Time'],\n",
    "        'History': row['History'],\n",
    "        'impressions': full_impression\n",
    "    })\n",
    "\n",
    "# 6. ìµœì¢… ê²°ê³¼ DataFrame ìƒì„±\n",
    "expanded_df = pd.DataFrame(expanded_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c988c817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â— -1 ë‰´ìŠ¤ê°€ 2ê°œ ì´ìƒì¸ í–‰ ìˆ˜: 0\n"
     ]
    }
   ],
   "source": [
    "# -1 ë ˆì´ë¸”ì´ 2ê°œ ì´ìƒì¸ í–‰ì„ ì°¾ëŠ”ë‹¤\n",
    "def count_label_1(imps):\n",
    "    return sum(1 for x in imps.split() if x.endswith('-1'))\n",
    "\n",
    "multi_label1_rows = expanded_df[expanded_df['impressions'].apply(count_label_1) > 1]\n",
    "print(f\"â— -1 ë‰´ìŠ¤ê°€ 2ê°œ ì´ìƒì¸ í–‰ ìˆ˜: {len(multi_label1_rows)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e48a3e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª -0 ë‰´ìŠ¤ ê°œìˆ˜ í†µê³„:\n",
      "num_label_0\n",
      "20    347727\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# impressionsì—ì„œ -0ë§Œ ì¶”ì¶œ\n",
    "def extract_label_0(imps):\n",
    "    return [x for x in imps.split() if x.endswith('-0')]\n",
    "\n",
    "# ëª¨ë“  í–‰ì´ ì •í™•íˆ 20ê°œì˜ -0 ë‰´ìŠ¤ë¥¼ ê°€ì§€ê³  ìˆëŠ”ì§€ í™•ì¸\n",
    "expanded_df['num_label_0'] = expanded_df['impressions'].apply(lambda x: len(extract_label_0(x)))\n",
    "print(\"ğŸ§ª -0 ë‰´ìŠ¤ ê°œìˆ˜ í†µê³„:\")\n",
    "print(expanded_df['num_label_0'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43451cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 347727/347727 [00:03<00:00, 104715.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 24ì‹œê°„ ì¡°ê±´ ë§Œì¡±í•˜ëŠ” í–‰ ìˆ˜: 0 / 347727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ì‹œê°„ ê¸°ì¤€ ê²€ì¦\n",
    "def check_all_within_24h(row):\n",
    "    time = row['Time']\n",
    "    for x in row['impressions'].split():\n",
    "        if x.endswith('-0'):\n",
    "            news_id = x.replace('-0', '')\n",
    "            pub_time = news_publish_map.get(news_id)\n",
    "            if not pub_time:\n",
    "                return False\n",
    "            if not (timedelta(0) <= (time - pub_time) <= timedelta(hours=24)):\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "within_check = expanded_df.progress_apply(check_all_within_24h, axis=1)\n",
    "print(f\"âœ… 24ì‹œê°„ ì¡°ê±´ ë§Œì¡±í•˜ëŠ” í–‰ ìˆ˜: {within_check.sum()} / {len(expanded_df)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
